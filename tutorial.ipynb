{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of Packages\n",
        "Restart session after installation to avoid numpy/scipy import errors"
      ],
      "metadata": {
        "id": "Vr-MimX0yfrq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_9855Ndjwc2y"
      },
      "outputs": [],
      "source": [
        "!pip install --prefer-binary pyscf\n",
        "!pip install openfermion==1.7\n",
        "!pip install openfermionpyscf\n",
        "!pip install opt_einsum\n",
        "\n",
        "# If you have access to GPU, some functions make use of python package CuPy. However, it is recommended to avoid this as it was only experimental.\n",
        "# !pip install cupy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the Github repository\n",
        "\n",
        "If you want to save it in google drive for future use and make updates, first mount the drive and change the directory to a folder inside drive, then clone the repo."
      ],
      "metadata": {
        "id": "ft-1GKd0FLxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/your-folder          #Change your-folder with the name of the folder in drive where you want to clone the repo\n",
        "\n",
        "!git clone https://github.com/Shashank-G-M/Perturbative_Trotter_Error.git\n",
        "\n",
        "%cd Perturbative_Trotter_Error"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fUfZC7luL_Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d25a255"
      },
      "source": [
        "## Importing the Modules and Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71282f94",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import scipy as sp\n",
        "from scipy.sparse.linalg import expm, eigs, eigsh\n",
        "from scipy.sparse import csc_matrix, csr_matrix, bsr_matrix\n",
        "from scipy.linalg import logm\n",
        "from scipy.optimize import minimize, Bounds\n",
        "from scipy.optimize import differential_evolution, NonlinearConstraint\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from itertools import product\n",
        "from itertools import accumulate\n",
        "from itertools import permutations, combinations\n",
        "\n",
        "from functools import reduce\n",
        "from joblib import Parallel, delayed\n",
        "from copy import copy\n",
        "from opt_einsum import contract\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "\n",
        "import openfermion as of\n",
        "from openfermion.hamiltonians import fermi_hubbard\n",
        "from openfermion.transforms import jordan_wigner, reverse_jordan_wigner\n",
        "from openfermion.linalg import qubit_operator_sparse\n",
        "from openfermion.linalg import get_number_preserving_sparse_operator\n",
        "from openfermion.utils import count_qubits, is_hermitian\n",
        "from openfermion import normal_ordered, chemist_ordered, commutator\n",
        "from openfermion import eigenspectrum\n",
        "from openfermion import get_sparse_operator\n",
        "from openfermion import FermionOperator, QubitOperator, MolecularData, hermitian_conjugated, get_molecular_data\n",
        "from openfermionpyscf import run_pyscf\n",
        "from openfermion.transforms import get_fermion_operator\n",
        "from openfermion import jw_hartree_fock_state\n",
        "from openfermion import MajoranaOperator\n",
        "from openfermion.hamiltonians import s_squared_operator, sz_operator, number_operator\n",
        "\n",
        "from pyscf import gto, scf, lo, fci\n",
        "from pyscf.cc.addons import spatial2spin\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "from pert_trotter.cost_utils import *\n",
        "from pert_trotter.tensor_utils import *\n",
        "from pert_trotter.ffrag_utils import *\n",
        "from pert_trotter.fock_utils import *\n",
        "from pert_trotter.ham_utils import *\n",
        "from pert_trotter.io_utils import *\n",
        "from pert_trotter.taper_utils import *\n",
        "from pert_trotter.trotter_utils import *\n",
        "from pert_trotter.fermi_frag import *\n",
        "from pert_trotter.error_pert import *\n",
        "from pert_trotter.qubit_utils import get_fc_group\n",
        "from pert_trotter.qubit_utils import get_qwc_group\n",
        "from pert_trotter.qubit_utils import get_greedy_grouping\n",
        "from pert_trotter.qubit_utils import Do_Qubit_Partitioning\n",
        "from pert_trotter import config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0gcB_iAVrG0"
      },
      "source": [
        "## Build molecular Hamiltonian\n",
        "\n",
        "All molecules except NH$_3$ are generated on the fly. NH$_3$ Hamiltonian, however, is loaded from the repository. To reproduce the results from the paper, use ```r=1``` for H$_2$, LiH, and BeH$_2$, and ```r=1.9``` for H$_2$O and NH$_3$.\n",
        "\n",
        "We demonstrate the usage for the case of LiH."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4489b978"
      },
      "outputs": [],
      "source": [
        "r = 1\n",
        "mol_name = 'lih'                                                                #Can be 'h2', 'beh2', 'h2o', and 'nh3'.\n",
        "if mol_name == 'nh3' and r == 1.9:\n",
        "  with open('data/mol_data/nh3_mol_data.pkl', 'rb') as in_file:\n",
        "    mol_data = pickle.load(in_file)\n",
        "  H = mol_data['FermionOperator']\n",
        "  num_elecs = mol_data['num_elecs']\n",
        "  n_qubits = count_qubits(H)\n",
        "  H_const, H_obt, H_tbt = get_chem_tensors(H)\n",
        "  H_obt_op, H_tbt_op = obt2op(H_obt), tbt2op(H_tbt)\n",
        "  Hchem = H_obt_op + H_tbt_op + H_const\n",
        "else:\n",
        "  H, num_elecs = obtain_OF_hamiltonian(mol_name, geometry=r)\n",
        "  n_qubits = count_qubits(H)\n",
        "  print (num_elecs, n_qubits)\n",
        "  H_const, H_obt, H_tbt = get_chem_tensors(H)\n",
        "  H_obt_op, H_tbt_op = obt2op(H_obt), tbt2op(H_tbt)\n",
        "  Hchem = H_obt_op + H_tbt_op + H_const\n",
        "\n",
        "config.mol_name = mol_name\n",
        "config.n_qubits = n_qubits\n",
        "config.num_elecs = num_elecs\n",
        "savepath = config.savepath\n",
        "\n",
        "JW_OF = jordan_wigner(Hchem)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate exact and approximate eigenstates in the full space"
      ],
      "metadata": {
        "id": "RNYMbVaFkjdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For non-tapered Hamiltonians\n",
        "\n",
        "Use this for smaller molecules i.e. exclude NH$_3$. Create the following directory to save the eigenstates.\\\n",
        "```data/JW_OF/mol_name```\\\n",
        "where ```mol_name``` should be replace by the name of the molecule for which data is being produced."
      ],
      "metadata": {
        "id": "OIDFjVl5knFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the exact eigenstates\n",
        "JW_OF_Array_sparse = qubit_operator_sparse(JW_OF)\n",
        "JW_OF_Array = JW_OF_Array_sparse.toarray()\n",
        "v, w = sp.linalg.eigh(JW_OF_Array)\n",
        "v0 = v[0]\n",
        "w0 = w[:,[0]]\n",
        "w0_sparse = sp.sparse.csc_matrix(w[:,[0]])\n",
        "\n",
        "# Generating the CISD approximated eigenstates\n",
        "proj_H, all_excitations = get_CI_proj_ham(JW_OF_Array, n_excitations = 2, ret_excitations = True, num_elecs = num_elecs, n_qubits = n_qubits)\n",
        "all_excitations_vec = np.zeros((2**n_qubits, len(all_excitations)))\n",
        "for i in range(len(all_excitations)):\n",
        "  all_excitations_vec[:,[i]] = occ_str_to_state(all_excitations[i])\n",
        "CISD_evals,CISD_evecs = np.linalg.eigh(proj_H)\n",
        "CISD_evecs_full_space = all_excitations_vec@CISD_evecs\n",
        "\n",
        "# Regordering the CISD states so that the first vector corresponds to S^2 = 0\n",
        "Ssq = s_squared_operator(n_qubits//2)\n",
        "Ssq_full_sparse = get_sparse_operator(Ssq, n_qubits)\n",
        "for i in range (len(CISD_evals)):\n",
        "  vec = sp.sparse.csc_matrix(CISD_evecs_full_space[:,[i]])\n",
        "  overlap = np.abs(vec.T*Ssq_full_sparse*vec)[0,0]\n",
        "  if np.round(overlap) == 0:\n",
        "    print ('Position of ground state of H with S^2 = 0: ', i)\n",
        "    break\n",
        "CISD_evecs_full_space[i], CISD_evecs_full_space[0] = CISD_evecs_full_space[0], CISD_evecs_full_space[i]\n",
        "CISD_evals[i], CISD_evals[0] = CISD_evals[0], CISD_evals[i]\n",
        "w0_sparse_CISD = sp.sparse.csc_matrix(CISD_evecs_full_space[:,[0]])\n",
        "\n",
        "print ('My CISD gs energy: ', CISD_evals[0])\n",
        "\n",
        "gs = CISD_evecs_full_space[:,[0]]\n",
        "\n",
        "\n",
        "#Saving the states. Ensure appropriate directories are available\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_v', 'wb') as out_file:\n",
        "  pickle.dump(v, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_w', 'wb') as out_file:\n",
        "  pickle.dump(w, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_CISD_evals', 'wb') as out_file:\n",
        "  pickle.dump(CISD_evals, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_CISD_evecs', 'wb') as out_file:\n",
        "  pickle.dump(CISD_evecs, out_file)\n",
        "with open('data/JW_OF/'+ mol_name + '/'+ mol_name + '_CISD_evecs_full_space', 'wb') as out_file:\n",
        "  pickle.dump(CISD_evecs_full_space, out_file)"
      ],
      "metadata": {
        "id": "NJPrD5hukfPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For tapered Hamiltonians\n",
        "\n",
        "Can be used for larger molecules. To reproduce the results for qubit based fragments of NH$_3$, use this method. Create the following directory to save the eigenstates.\\\n",
        "```data/tapered_JW_OF/mol_name```\\\n",
        "where ```mol_name``` should be replace by the name of the molecule for which data is being produced."
      ],
      "metadata": {
        "id": "mriltbl1l5Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_ham = False                      # Set it to True when you have already saved the tapered Hamiltonian\n",
        "save_ham = True                      # Set it to True when you want to save the tapered Hamiltonian\n",
        "GPU = False                           # Experimental.\n",
        "\n",
        "\n",
        "if load_ham == True:\n",
        "  savepath = 'data/'\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_ham_ndarray', 'rb') as in_file:\n",
        "    tapered_JW_OF_array = pickle.load(in_file)\n",
        "  # tapered_JW_OF_sarray = sp.sparse.csc_matrix(tapered_JW_OF_array)\n",
        "  ref_det = '1'*num_elecs + '0'*(n_qubits-num_elecs)\n",
        "else:\n",
        "  ref_det = '1'*num_elecs + '0'*(n_qubits-num_elecs)\n",
        "  tapered_JW_OF = taper_qubits(JW_OF, n_qubits, int(num_elecs/2), int(num_elecs/2))\n",
        "  tapered_JW_OF_sarray = get_sparse_operator(tapered_JW_OF, n_qubits-2)\n",
        "  tapered_JW_OF_array = np.real(tapered_JW_OF_sarray).toarray()\n",
        "  if save_ham == True:\n",
        "    savepath = 'data/'\n",
        "    with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_ham_ndarray', 'wb') as out_file:\n",
        "      pickle.dump(tapered_JW_OF_array, out_file)\n",
        "\n",
        "\n",
        "if GPU == True:\n",
        "  tapered_JW_OF_cparray = cp.asarray(tapered_JW_OF_array)\n",
        "  tap_v_cp, tap_w_cp = cp.linalg.eigh(tapered_JW_OF_cparray)\n",
        "  tap_v = cp.asnumpy(tap_v_cp)\n",
        "  tap_w = cp.asnumpy(tap_w_cp)\n",
        "else:\n",
        "  tap_v, tap_w = np.linalg.eigh(tapered_JW_OF_array)\n",
        "\n",
        "tap_w0_sparse = sp.sparse.csc_matrix(tap_w[:,[0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tapered_JW_OF_sarray = sp.sparse.csc_matrix(tapered_JW_OF_array)\n",
        "tapered_CI_states = get_tapered_CI_states(ref_det, [0,1,2], preserve_Sz=False)\n",
        "CIproj_tap_JW_OF_sarray = tapered_CI_states.T*tapered_JW_OF_sarray*tapered_CI_states\n",
        "\n",
        "CIproj_tap_v0 = eigsh(CIproj_tap_JW_OF_sarray, k = 1, which='SA', return_eigenvectors=False)\n",
        "tap_v0 = eigsh(tapered_JW_OF_sarray, k = 1, which='SA', return_eigenvectors=False)\n",
        "\n",
        "# print ('Error due to tapering: ', np.abs(sym_v[0]-tap_v0))\n",
        "# print ('Error due to CISD + tapering: ', np.abs(sym_v[0]-CIproj_tap_v0))\n",
        "# print ('Error between CISD and tapered CISD: ', np.abs(CIproj_tap_v0 - H_NSz_CISD_v[0]))\n",
        "\n",
        "tap_CISD_evals, tap_CISD_evecs = np.linalg.eigh(CIproj_tap_JW_OF_sarray.toarray())\n",
        "tap_CISD_evecs_full_space = tapered_CI_states*tap_CISD_evecs\n",
        "\n",
        "\n",
        "with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_v', 'wb') as in_file:\n",
        "  pickle.dump(tap_v, in_file)\n",
        "\n",
        "dump_ndarray_h5(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_w.h5', tap_w)\n",
        "\n",
        "\n",
        "with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evals', 'wb') as in_file:\n",
        "  pickle.dump(tap_CISD_evals, in_file)\n",
        "\n",
        "with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evecs_full_space', 'wb') as in_file:\n",
        "  pickle.dump(tap_CISD_evecs_full_space, in_file)"
      ],
      "metadata": {
        "id": "w75_cxoml7IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the eigenstates and eigenvalues"
      ],
      "metadata": {
        "id": "CxdUIOBfgGDb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvXddtgk74Qh"
      },
      "source": [
        "## Building projector: $N = \\eta,\\ S_z=0,\\ $and$\\ S^2 = 0$\n",
        "\n",
        "Unliike qubit fragments, fermionic fragments have the same symmetry as the Hamiltonian. This enables us to perform calculations in the symmetry subspace of the ground state of the Hamiltonian. In this section, we build the projector onto this subspace. We also project CISD approximated eigenstates onto this subspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwPgkSbu0AVD"
      },
      "outputs": [],
      "source": [
        "Ssq = s_squared_operator(n_qubits//2)\n",
        "Ssq_sparse = get_number_preserving_sparse_operator(Ssq, n_qubits, num_elecs, spin_preserving=True)\n",
        "\n",
        "Ssq_array = Ssq_sparse.toarray()\n",
        "\n",
        "Ssq_v, Ssq_w = np.linalg.eigh(Ssq_array)\n",
        "Ssq_w_sparse = sp.sparse.csc_matrix(Ssq_w)\n",
        "# Ssq_w0_sparse = sp.sparse.csc_matrix(Ssq_w[:,[0]])\n",
        "\n",
        "counter = 0\n",
        "for i in range(len(Ssq_v)):\n",
        "    if Ssq_v[i]<= 0.01:\n",
        "        counter += 1\n",
        "non_cisd_dim = counter\n",
        "\n",
        "Ssq_evals, NSz2SSq_Proj = Ssq_v[:counter], Ssq_w[:, :counter].T\n",
        "NSz2SSq_Proj_sparse = sp.sparse.csc_matrix(NSz2SSq_Proj)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Projeector within CISD space\n",
        "Ssq_CISD_sparse = get_number_preserving_sparse_operator(Ssq, n_qubits, num_elecs, spin_preserving=True, excitation_level=2)\n",
        "Ssq_CISD_array = Ssq_CISD_sparse.toarray()\n",
        "\n",
        "Ssq_CISD_v, Ssq_CISD_w = np.linalg.eigh(Ssq_CISD_array)\n",
        "Ssq_CISD_w_sparse = sp.sparse.csc_matrix(Ssq_CISD_w)\n",
        "Ssq_CISD_w0_sparse = sp.sparse.csc_matrix(Ssq_CISD_w[:,[0]])\n",
        "\n",
        "counter = 0\n",
        "for i in range(len(Ssq_CISD_v)):\n",
        "    if Ssq_CISD_v[i]<= 0.01:\n",
        "        counter += 1\n",
        "cisd_dim = counter\n",
        "\n",
        "Ssq_CISD_evals, NSz2SSq_CISD_Proj = Ssq_CISD_v[:counter], Ssq_CISD_w[:, :counter].T\n",
        "NSz2SSq_CISD_Proj_sparse = sp.sparse.csc_matrix(NSz2SSq_CISD_Proj)\n",
        "\n",
        "\n",
        "\n",
        "def get_projected_sparse_op(H_OF = FermionOperator | QubitOperator, n_qubits = n_qubits, num_elecs = num_elecs, spin_preserving = True, excitation_level = None, reference_determinant=None): #H_OF is a FermioOperator in full space\n",
        "  if type(H_OF) == QubitOperator:\n",
        "    H_OF = normal_ordered(reverse_jordan_wigner(H_OF))\n",
        "  first_projected_op = get_number_preserving_sparse_operator(H_OF, n_qubits, num_elecs, spin_preserving=spin_preserving, excitation_level=excitation_level, reference_determinant=reference_determinant)\n",
        "  if excitation_level == None:\n",
        "    return NSz2SSq_Proj_sparse*first_projected_op*NSz2SSq_Proj_sparse.T\n",
        "  elif excitation_level == 2:\n",
        "    return NSz2SSq_CISD_Proj_sparse*first_projected_op*NSz2SSq_CISD_Proj_sparse.T\n",
        "  else:\n",
        "    print ('Invalid excitation level. Should be 2 or None.')\n",
        "    return None\n",
        "\n",
        "def get_projected_sparse_vec(vec, CISD = False): # vec must be a M x 1 sparse operator where M is the dimension of N = \\eta, Sz = 0 subspace.\n",
        "  if CISD == False:\n",
        "    return NSz2SSq_Proj_sparse*vec\n",
        "  else:\n",
        "    return NSz2SSq_CISD_Proj_sparse*vec\n",
        "\n",
        "\n",
        "\n",
        "H_Proj_sparse = get_projected_sparse_op(Hchem, n_qubits, num_elecs, spin_preserving=True)\n",
        "H_Proj_Array = H_Proj_sparse.toarray()\n",
        "#Hamiltonian eigenstates projected onto number of electrons = num_elecs, Sz = 0, and S^2 = 0 subspace\n",
        "sym_v, sym_w = np.linalg.eigh(H_Proj_Array)\n",
        "sym_w0_sparse = sp.sparse.csc_matrix(sym_w[:,[0]])\n",
        "\n",
        "\n",
        "H_NSz_CISD = get_number_preserving_sparse_operator(Hchem, n_qubits, num_elecs, spin_preserving=True, excitation_level=2)\n",
        "H_NSz_CISD_v, H_NSz_CISD_w = np.linalg.eigh(H_NSz_CISD.toarray())\n",
        "correct_zeros = np.zeros((len(Ssq_v) - len(H_NSz_CISD_v), len(H_NSz_CISD_v)))\n",
        "H_NSz_CISD_full_space_w = np.vstack((H_NSz_CISD_w, correct_zeros))\n",
        "#CISD approx to Hamiltonian eigenstates projected onto number of electrons = num_elecs, Sz = 0, and S^2 = 0 subspace\n",
        "H_NSzSsq_CISD_full_space_w = NSz2SSq_Proj@H_NSz_CISD_full_space_w\n",
        "H_NSzSsq_CISD_full_space_w0_sparse = sp.sparse.csc_matrix(H_NSzSsq_CISD_full_space_w[:,[0]])\n",
        "\n",
        "#CISD approximated eigenstates which do not have S^2 = 0 will be columns of H_NSzSsq_CISD_full_space_w with ~ 0 everywhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFw8vk0gfw7P"
      },
      "source": [
        "## Generating Hamiltonian Partitioning\n",
        "\n",
        "We generate 4 kinds of qubit Hamiltonian fragments (QWCSI, QWCLF, FCSI, FCLF) and 5 kinds of fermionic Hamiltonian fragments (LRLCU, GFROLCU, LR, GFRO, SDGFRO). Refer to Appendix A of the paper for more detials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDLiESzhoGR2"
      },
      "source": [
        "### Non tapered\n",
        "\n",
        "Before proceeding, create the following tree of directories.\\\n",
        "```data/method/sparse arrays/Full space```\n",
        "\n",
        "For example, to generate QWCSI fragments, following directories should be present.\\\n",
        "```data/qwcsi/sparse arrays/Full space```\n",
        "\n",
        "For large systems, uncomment the last line to store the data as a hdf5 file instad of a pickle dump."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nvQ05i1Kf0WQ"
      },
      "outputs": [],
      "source": [
        "save, load, spacial = True, False, True\n",
        "\n",
        "method = 'lrlcu'\n",
        "\n",
        "if method in ['lr', 'gfro', 'lrlcu', 'gfrolcu', 'sdgfro']:\n",
        "  fragments_list_sarray = Do_Fermi_Partitioning (Hchem, type = method, tol=1e-6, spacial = spacial, save = save, load = load, projector_func = get_projected_sparse_op)\n",
        "else:\n",
        "  fragments_list = Do_Qubit_Partitioning (JW_OF, type = method)\n",
        "  #IMPORTANT!!!!! For Sorted insertion based methods (qwcsi and fcsi), manually add the appropriate identity operator to the first fragment, as they lack the identity.\n",
        "  if method in ['qwcsi', 'fcsi']:\n",
        "    fragments_list[0] += JW_OF.constant*QubitOperator.identity()\n",
        "  with open(savepath + method + '/' + mol_name +'_'  + method + '_frag_ops.pkl', 'wb') as out_file:\n",
        "    pickle.dump(fragments_list, out_file)\n",
        "  fragments_list_sarray = [get_sparse_operator(frag, n_qubits) for frag in fragments_list]\n",
        "\n",
        "\n",
        "sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'wb') as in_file:\n",
        "  pickle.dump(fragments_list_sarray, in_file)\n",
        "# dump_list_of_sparse2_h5(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5', fragments_list_sarray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25fIR6NgoIhp"
      },
      "source": [
        "### Tapered\n",
        "Before proceeding, create the following tree of directories.\\\n",
        "```data/tapered_method/sparse arrays/Full space```\n",
        "\n",
        "For example, to generate QWCSI fragments, following directories should be present.\\\n",
        "```data/tapered_qwcsi/sparse arrays/Full space```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFGdt5TXwqAx"
      },
      "outputs": [],
      "source": [
        "tapered_JW_OF = taper_qubits(JW_OF, n_qubits, int(num_elecs/2), int(num_elecs/2))\n",
        "method = 'qwcsi'\n",
        "fragments_list = Do_Qubit_Partitioning(tapered_JW_OF, type = method)\n",
        "#IMPORTANT!!!!! For Sorted insertion based methods (qwcsi and fcsi), manually add the appropriate identity operator to the first fragment, as they lack the identity.\n",
        "if method in ['qwcsi', 'fcsi']:\n",
        "  fragments_list[0] += tapered_JW_OF.constant*QubitOperator.identity()\n",
        "with open(savepath + 'tapered_' + method + '/' + mol_name +'_'  + method + '_frag_ops.pkl', 'wb') as out_file:\n",
        "  pickle.dump(fragments_list, out_file)\n",
        "\n",
        "\n",
        "fragments_list_sarray = [get_sparse_operator(frag, n_qubits-2) for frag in fragments_list]\n",
        "sarray_savepath = savepath + 'tapered_' + method + '/sparse arrays/'\n",
        "dump_list_of_sparse2_h5(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5', fragments_list_sarray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeMAOPwpmPnu"
      },
      "source": [
        "## Calculate exact and approximate perturbative Trotter error $(\\varepsilon_2,\n",
        " \\varepsilon_{\\text{app}})$\n",
        "\n",
        " Assumes you have generated and stored relevant fragments and eigenstates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNjCIfKFvY6e"
      },
      "source": [
        "### For fermionic fragments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl5luwHsvaLZ"
      },
      "outputs": [],
      "source": [
        "method = 'lrlcu'\n",
        "sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "\n",
        "\n",
        "try:\n",
        "  fragments_list_sarray = list(load_h5_list_of_sparse(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5'))\n",
        "except FileNotFoundError:\n",
        "  with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "    fragments_list_sarray = list(pickle.load(in_file))\n",
        "\n",
        "\n",
        "v, w, w0_sparse = sym_v, sym_w, sym_w0_sparse\n",
        "CISD_v, CISD_w, CISD_w0_sparse = H_NSz_CISD_v, H_NSzSsq_CISD_full_space_w, H_NSzSsq_CISD_full_space_w0_sparse\n",
        "\n",
        "\n",
        "\n",
        "#Second Order Trotter\n",
        "print ('Full space (\\u03F5_2):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)\n",
        "\n",
        "\n",
        "print ('\\n\\nCISD space (\\u03F5_app):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, CISD_w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AweGzufBvjJ0"
      },
      "source": [
        "### For qubit fragments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laiZav86e6e_"
      },
      "outputs": [],
      "source": [
        "method = 'qwcsi'\n",
        "mol_name = 'lih'\n",
        "istapered = False                       # Setting this to True assumes you have generated and saved the tapered data for the current molecule\n",
        "\n",
        "\n",
        "if istapered == True:\n",
        "  sarray_savepath = savepath + 'tapered_' + method + '/sparse arrays/'\n",
        "  fragments_list_sarray = load_h5_list_of_sparse(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5')\n",
        "\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_v', 'rb') as in_file:\n",
        "    v = pickle.load(in_file)\n",
        "  w = load_h5_ndarray(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_w.h5')\n",
        "  w0_sparse = sp.sparse.csc_matrix(w[:,[0]])\n",
        "\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evals', 'rb') as in_file:\n",
        "    CISD_v = pickle.load(in_file)\n",
        "  with open(savepath + 'tapered_JW_OF/' + mol_name + '/' + mol_name + '_tap_CISD_evecs_full_space', 'rb') as in_file:\n",
        "    CISD_w = pickle.load(in_file)\n",
        "  CISD_w0_sparse = sp.sparse.csc_matrix(CISD_w[:,[0]])\n",
        "else:\n",
        "  sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "  with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "    fragments_list_sarray = pickle.load(in_file)\n",
        "\n",
        "  with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_v', 'rb') as in_file:\n",
        "    v = pickle.load(in_file)\n",
        "\n",
        "  try:\n",
        "    w = load_h5_ndarray(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_w.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_w', 'rb') as in_file:\n",
        "      w = pickle.load(in_file)\n",
        "  w0_sparse = sp.sparse.csc_matrix(w[:,[0]])\n",
        "\n",
        "  with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_CISD_evals', 'rb') as in_file:\n",
        "    CISD_v = pickle.load(in_file)\n",
        "  with open(savepath + 'JW_OF/' + mol_name + '/' + mol_name + '_CISD_evecs_full_space', 'rb') as in_file:\n",
        "    CISD_w = pickle.load(in_file)\n",
        "  CISD_w0_sparse = sp.sparse.csc_matrix(CISD_w[:,[0]])\n",
        "\n",
        "\n",
        "#Second order Trotter\n",
        "print ('Full space (\\u03F5_2):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)\n",
        "\n",
        "print ('\\n\\nCISD space (\\u03F5_app):')\n",
        "v2_contri = efficient_v2_contri_2_order_Trotter(fragments_list_sarray, CISD_w0_sparse)\n",
        "print ('2nd order Trotter v2 contribution: ', v2_contri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9WDZJIfkfqZ"
      },
      "source": [
        "## Evaluating $\\alpha$\n",
        "\n",
        "Refer to Eq. (4) of the paper for the definition of $\\alpha$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import cupy as cp\n",
        "# # import cupyx.scipy.sparse as cusparse\n",
        "# # from cupyx.scipy.sparse.linalg import eigsh as gpu_eigsh\n",
        "\n",
        "method = 'lrlcu'\n",
        "mol_name = 'lih'\n",
        "istapered = False\n",
        "\n",
        "sarray_savepath = savepath + method + '/sparse arrays/'\n",
        "\n",
        "if istapered == True:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle\n",
        "else:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(sarray_savepath + 'Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(sarray_savepath + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle.load(in_file)\n",
        "\n",
        "# fragments_list_sarray_gpu = [cusparse.csc_matrix(frag, dtype=cp.complex128) for frag in fragments_list_sarray]\n",
        "\n",
        "alpha = Trotter_2nd_order_alpha_error(fragments_list_sarray, gpu=False)\n",
        "print (alpha)"
      ],
      "metadata": {
        "id": "bkgBNLLA_QH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-OvwBjNo10u"
      },
      "source": [
        "## Working with Trotter Unitary\n",
        "\n",
        "Get the effective Hamiltonian from U_trotter by applying matrix logarithm to U_trotter unitary. This cell also calculates all other relevant quantities concerning U_trotter and $\\hat{H}_{\\text{eff}}$ like $\\alpha_e$ (see Eq. (10) in paper),$\\ \\varepsilon,\\ E_0,\\ E_0^{T},\\ \\Delta E_T,\\ $and time step $t$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method = 'lrlcu'\n",
        "mol_name = 'lih'\n",
        "istapered = False\n",
        "\n",
        "if istapered == True:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + 'tapered_' + method + '/sparse arrays/Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle\n",
        "else:\n",
        "  try:\n",
        "    fragments_list_sarray = load_h5_list_of_sparse(savepath + method + '/sparse arrays/' + 'Full space/' + mol_name + '_sarray.h5')\n",
        "  except FileNotFoundError:\n",
        "    with open(savepath + method + '/sparse arrays/' + 'Full space/' + mol_name + '_sarray.pkl', 'rb') as in_file:\n",
        "      fragments_list_sarray = pickle.load(in_file)\n",
        "\n",
        "print ('Check point 1')\n",
        "\n",
        "frags_len = len(fragments_list_sarray)\n",
        "\n",
        "Happ = sp.sparse.csc_matrix(sum(fragments_list_sarray))\n",
        "v0 = sp.sparse.linalg.eigsh(Happ, k=1, which='SA', tol = 1e-8, return_eigenvectors=False)[0]\n",
        "spec_norm = np.abs(sp.sparse.linalg.eigsh(Happ, k=1, which='LM', tol = 1e-8, return_eigenvectors=False)[0])\n",
        "dt = 1/spec_norm\n",
        "\n",
        "print ('Check point 2')\n",
        "\n",
        "\n",
        "U_trotter = sp.sparse.eye(Happ.shape[0], format = 'csc')\n",
        "counter = 1\n",
        "for frag in fragments_list_sarray:\n",
        "  print ('scipy exponentiation initiated (via sp.sparse.linalg.expm)')\n",
        "  U_trotter_frag = sp.sparse.linalg.expm(-1.j*frag*dt/2)                        #dt/2 because we are eventually going to construct second order Trotter unitary\n",
        "  print ('scipy exponentiation complete')\n",
        "\n",
        "  U_trotter = U_trotter*U_trotter_frag\n",
        "\n",
        "  print (f'Loop : {counter}/{frags_len} \\n')\n",
        "  counter += 1\n",
        "U_trotter_array = U_trotter.toarray()\n",
        "\n",
        "print ('Check point 3')\n",
        "\n",
        "\n",
        "U_trotter_array_2nd_order = U_trotter_array@U_trotter_array.T\n",
        "H_eff_array_2nd_order = 1.j*sp.linalg.logm(U_trotter_array_2nd_order)/(dt)\n",
        "H_eff_sarray_2nd_order = sp.sparse.csc_matrix(H_eff_array_2nd_order)\n",
        "\n",
        "print ('Check point 4')\n",
        "\n",
        "v0_eff = sp.sparse.linalg.eigsh(H_eff_sarray_2nd_order, k=1, which='SA', tol = 1e-8, return_eigenvectors=False)[0]\n",
        "\n",
        "DE_T = np.abs(v0_eff - v0)\n",
        "epsilon = DE_T/dt**2\n",
        "\n",
        "print ('Epsilon = ', epsilon)\n",
        "\n",
        "U_trotter_sarray_2nd_order = sp.sparse.csc_matrix(U_trotter_array_2nd_order)\n",
        "\n",
        "U_exact_sarray = sp.sparse.linalg.expm(-1.j*Happ*dt)\n",
        "alpha_e_op = U_exact_sarray - U_trotter_sarray_2nd_order\n",
        "alpha_e = np.abs(sp.sparse.linalg.eigs(alpha_e_op, k=1, which='LM', return_eigenvectors = False)[0])\n",
        "alpha_e = alpha_e/dt**3\n",
        "\n",
        "print ('alpha_e = ', alpha_e)\n",
        "\n",
        "print ('dt = ', dt)\n",
        "\n",
        "H_eff_info = {'mol_name': mol_name, 'fragmentation': method, 'E0': v0, 'E0_eff': v0_eff, 'DeltaE_T': DE_T, 'epsilon': epsilon, 'alpha_e': alpha_e, 'time_step': dt}\n",
        "\n",
        "print (H_eff_info)\n",
        "\n",
        "with open(savepath + method + '/sparse arrays/' + 'Full space/' + mol_name + '_H_eff_info.pkl', 'wb') as out_file:\n",
        "  pickle.dump(H_eff_info, out_file)"
      ],
      "metadata": {
        "id": "U-AhL7-AjwXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaNvGpPnWII"
      },
      "source": [
        "## Correlation Plots\n",
        "\n",
        "Producing figure 1 from paper using tables provided in the paper. All the values in the tables can be generated by using the cells above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "a = 0\n",
        "b = 5\n",
        "\n",
        "c = 0\n",
        "d = 9\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 13})\n",
        "\n",
        "eps_v2_exact = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032480968, 0.002163644, 0.0030127983, 0.00244501, 0.003300397, 0.003386966, 0.04718898, 0.049932963, 0.01818811],\n",
        "    [0.013731045, 0.011020856, 0.02272475, 0.008758768, 0.009295491, 0.009634088, 0.028728955, 0.03341760, 0.0195634463],\n",
        "    [0.006219993, 0.004758676, 0.02356259, 0.00284857, 0.023670257, 0.02508310, 0.14204710, 0.128333, 0.02582307],\n",
        "    [0.01148842, 0.00997528, 0.089486699, 0.0152268, 0.019967049, 0.0198038678, 0.165155677, 0.137869267, 0.029367855]\n",
        "])\n",
        "eps_v2_exact = eps_v2_exact[a:b, c:d]\n",
        "\n",
        "\n",
        "eps_v2_app = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032552085, 0.00217699, 0.003022389, 0.002458995, 0.003304652, 0.003386966, 0.047186947, 0.049938408, 0.01818605],\n",
        "    [0.01382715, 0.011163517, 0.02250201, 0.008934338, 0.00949379, 0.009828665, 0.0289486374, 0.03363103, 0.0197738062],\n",
        "    [0.009778753, 0.00804437, 0.019877499, 0.006027446, 0.032246767, 0.035204712, 0.177783456, 0.16698050, 0.03484607],\n",
        "    [0.01311337, 0.01565665, 0.07786943, 0.01048145, 0.033328745, 0.0344475977, 0.23392218, 0.20878358, 0.0474685]\n",
        "])\n",
        "eps_v2_app = eps_v2_app[a:b, c:d]\n",
        "\n",
        "eps = np.array([\n",
        "    [0.0033079, 0.003308, 0.00330791, 0.0033079, 0.0033079, 0.0033079, 0.0028361, 0.00284251, 0.0030695],\n",
        "    [0.0032497, 0.002168, 0.00301, 0.002449, 0.00330, 0.00339, 0.047050, 0.049808, 0.018174],\n",
        "    [0.0137506, 0.011032, 0.022746, 0.008771, 0.009307, 0.009646, 0.02869, 0.03334, 0.01956],\n",
        "    [0.006223, 0.004761, 0.023572, 0.002850, 0.023673, 0.025086, 0.142037, 0.128308, 0.025828],\n",
        "    [0.011495, 0.009978, 0.08958, 0.015229, 0.019971, 0.019808, 0.165136, 0.1378337, 0.0293757]\n",
        "])\n",
        "eps = eps[a:b, c:d]\n",
        "\n",
        "\n",
        "# Creating subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y = eps.flatten()\n",
        "y = y/max(y)\n",
        "x1 = eps_v2_exact.flatten()\n",
        "x1 = x1/max(x1)\n",
        "x2 = eps_v2_app.flatten()\n",
        "x2 = x2/max(x2)\n",
        "\n",
        "\n",
        "#Obtaining correlation coefficients\n",
        "r1, _ = pearsonr(list(x1), list(y))\n",
        "r2, _ = pearsonr(list(x2), list(y))\n",
        "\n",
        "\n",
        "\n",
        "axes[1].scatter(x1, y, alpha=0.7, marker='.', c='r', label = '$\\epsilon_{\\\\text{2}}$ correlation = %.2f'%r1)\n",
        "axes[1].scatter(x2, y, alpha=0.7, marker='*', c='b', label = '$\\epsilon_{\\\\text{app}}$ correlation = %.2f'%r2)\n",
        "\n",
        "axes[1].set_xlim(0,1.1)\n",
        "axes[1].set_ylim(0,1.1)\n",
        "\n",
        "# Fit and plot a trendline (optional)\n",
        "m1, b1 = np.polyfit(x1, y, 1)\n",
        "axes[1].plot(x1, m1 * x1 + b1, color='red', alpha = 0.3, linestyle='-')\n",
        "\n",
        "m2, b2 = np.polyfit(x2, y, 1)\n",
        "axes[1].plot(x2, m2 * x2 + b2, color='blue', linestyle='-')\n",
        "\n",
        "axes[1].set_xlabel(\"Perturbation based Trotter error\", fontsize=14)\n",
        "# axes[1].set_ylabel(\"Exact Trotter error ($\\epsilon$)\", fontsize=14)\n",
        "axes[1].legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Time steps\n",
        "dt = np.array([\n",
        "    [0.908141],\n",
        "    [0.128461],\n",
        "    [0.064592],\n",
        "    [0.01337],\n",
        "    [0.018123]\n",
        "])\n",
        "\n",
        "\n",
        "# alpha\n",
        "alpha = np.array([\n",
        "    [0.019917, 0.016297, 0.02223, 0.016297, 0.016296, 0.016297, 0.015327, 0.015319, 0.015776],\n",
        "    [1.071179, 0.261750, 0.62713, 0.260248, 0.130109, 0.117276, 0.523921, 0.461217, 0.230157],\n",
        "    [4.22008, 1.023566, 4.96021, 0.986088, 0.580984, 0.550564, 2.359864, 2.03442, 1.12533],\n",
        "    [79.40951, 28.73135, 181.5595, 27.85735, 15.30047, 15.05653, 52.37091, 48.27083, 27.88276],\n",
        "    [51.66221, 16.36468, 65.99235, 16.0206, 7.81380, 7.640403, 28.428811, 25.78507, 14.50743]\n",
        "])\n",
        "alpha = alpha[a:b, c:d]\n",
        "alpha_dt2 = alpha\n",
        "\n",
        "\n",
        "alpha_e = np.array([\n",
        "    [0.018603, 0.011459, 0.01860, 0.011459, 0.011459, 0.011459, 0.01111, 0.01111, 0.011299],\n",
        "    [0.22472, 0.180268, 0.2506, 0.180367, 0.10034, 0.09980, 0.07223, 0.08325, 0.06039],\n",
        "    [0.672117, 0.75365, 0.80521, 0.756595, 0.42353, 0.42957, 0.29231, 0.34624, 0.35711],\n",
        "    [23.25445, 23.42057, 46.44921, 23.44921, 1.84682, 11.86137, 15.66702, 14.87779, 12.86540],\n",
        "    [11.22894, 11.22535, 15.86476, 11.21336, 6.552417, 6.559554, 7.254640, 7.048216, 6.025332]\n",
        "])\n",
        "alpha_e = alpha_e[a:b, c:d]\n",
        "alpha_e_dt2 = alpha_e\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yy = eps.flatten()\n",
        "yy = yy/max(yy)\n",
        "xx1 = alpha_dt2.flatten()\n",
        "xx1 = xx1/max(xx1)\n",
        "xx2 = alpha_e_dt2.flatten()\n",
        "xx2 = xx2/max(xx2)\n",
        "\n",
        "\n",
        "rr1, _ = pearsonr(list(xx1), list(yy))\n",
        "rr2, _ = pearsonr(list(xx2), list(yy))\n",
        "\n",
        "\n",
        "axes[0].scatter(xx1, yy, alpha=0.7, marker='.', c='r', label = '$\\\\alpha$ correlation = %.2f'%rr1)\n",
        "axes[0].scatter(xx2, yy, alpha=0.7, marker='*', c='b', label = '$\\\\alpha_e$ correlation = %.2f'%rr2)\n",
        "\n",
        "axes[0].set_xlim(0,1.1)\n",
        "axes[0].set_ylim(0,1.1)\n",
        "\n",
        "# Fit and plot a trendline (optional)\n",
        "m1, b1 = np.polyfit(xx1, yy, 1)\n",
        "axes[0].plot(xx1, m1 * xx1 + b1, color='red', alpha = 0.7, linestyle='-')\n",
        "\n",
        "m2, b2 = np.polyfit(xx2, yy, 1)\n",
        "axes[0].plot(xx2, m2 * xx2 + b2, color='blue', linestyle='-')\n",
        "\n",
        "axes[0].set_xlabel(\"Operator norm based Trotter error\", fontsize=14)\n",
        "axes[0].set_ylabel(\"Exact Trotter error ($\\epsilon$)\", fontsize=14)\n",
        "axes[0].legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "axes[1].text(-0.01, 1.02, \"a)\", transform=axes[1].transAxes, fontsize=14, fontweight=\"bold\", va=\"top\", ha=\"right\")\n",
        "axes[0].text(-0.01, 1.02, \"b)\", transform=axes[0].transAxes, fontsize=14, fontweight=\"bold\", va=\"top\", ha=\"right\")\n",
        "\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "# #To save the figure, uncomment the following if working on google colab\n",
        "# from google.colab import files\n",
        "# plt.savefig('Trotter_error_correlation_plot.pdf')\n",
        "# files.download('Trotter_error_correlation_plot.pdf')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dXIhbnZNbeVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQWV4WPAICN"
      },
      "source": [
        "## T gate counts\n",
        "Calculate upperbound on T gates needed to reach chemical accuracy in obtaining ground state energy of molecular Hamiltonians under QPE + Trotter approximation algorithm for various Hamiltonian fragmentation techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the best T gates by sampling initial values for optimizer\n",
        "\n",
        "Since the function that optimize T gate count is highly sensitive to the inital parameters, we optimize T gates for various initial parameters on a grid to get a better estimate."
      ],
      "metadata": {
        "id": "AzvUhxl8_dIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGrjCgnUAJ1l"
      },
      "outputs": [],
      "source": [
        "eps_v2_exact = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032480968, 0.002163644, 0.0030127983, 0.00244501, 0.003300397, 0.003386966, 0.04718898, 0.049932963, 0.01818811],\n",
        "    [0.013731045, 0.011020856, 0.02272475, 0.008758768, 0.009295491, 0.009634088, 0.028728955, 0.03341760, 0.0195634463],\n",
        "    [0.006219993, 0.004758676, 0.02356259, 0.00284857, 0.023670257, 0.02508310, 0.14204710, 0.128333, 0.02582307],\n",
        "    [0.01148842, 0.00997528, 0.089486699, 0.0152268, 0.019967049, 0.0198038678, 0.165155677, 0.137869267, 0.029367855]\n",
        "])\n",
        "\n",
        "\n",
        "eps_v2_app = np.array([\n",
        "    [0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0032412139, 0.0027744548, 0.002780725, 0.00300493],\n",
        "    [0.0032552085, 0.00217699, 0.003022389, 0.002458995, 0.003304652, 0.003386966, 0.047186947, 0.049938408, 0.01818605],\n",
        "    [0.01382715, 0.011163517, 0.02250201, 0.008934338, 0.00949379, 0.009828665, 0.0289486374, 0.03363103, 0.0197738062],\n",
        "    [0.009778753, 0.00804437, 0.019877499, 0.006027446, 0.032246767, 0.035204712, 0.177783456, 0.16698050, 0.03484607],\n",
        "    [0.01311337, 0.01565665, 0.07786943, 0.01048145, 0.033328745, 0.0344475977, 0.23392218, 0.20878358, 0.0474685]\n",
        "])\n",
        "\n",
        "\n",
        "alpha = np.array([\n",
        "    [0.019917, 0.016297, 0.02223, 0.016297, 0.016296, 0.016297, 0.015327, 0.015319, 0.015776],\n",
        "    [1.071179, 0.261750, 0.62713, 0.260248, 0.130109, 0.117276, 0.523921, 0.461217, 0.230157],\n",
        "    [4.22008, 1.023566, 4.96021, 0.986088, 0.580984, 0.550564, 2.359864, 2.03442, 1.12533],\n",
        "    [79.40951, 28.73135, 181.5595, 27.85735, 15.30047, 15.05653, 52.37091, 48.27083, 27.88276],\n",
        "    [51.66221, 16.36468, 65.99235, 16.0206, 7.81380, 7.640403, 28.428811, 25.78507, 14.50743]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mol_names = ['h2', 'lih', 'beh2', 'h2o', 'nh3']\n",
        "all_methods = ['qwclf', 'qwcsi', 'fclf', 'fcsi', 'lrlcu', 'gfrolcu', 'lr', 'gfro', 'sdgfro']\n",
        "rs = [1, 1, 1, 1.9, 1.9]\n",
        "scale = 1e8\n",
        "Tot_Err = 1.6e-3\n",
        "already_tapered = False\n",
        "n_samps = 40\n",
        "\n",
        "#Arrays to store T gate estimates\n",
        "eps_v2_exact_T_count = np.zeros((len(mol_names), len(all_methods)))\n",
        "eps_v2_app_T_count = np.zeros((len(mol_names), len(all_methods)))\n",
        "alpha_T_count = np.zeros((len(mol_names), len(all_methods)))\n",
        "\n",
        "#Finding T gates for all the samples in one run\n",
        "for i, mol_name in enumerate(mol_names):\n",
        "  r = rs[i]\n",
        "  H, num_elecs = obtain_OF_hamiltonian(mol_name, geometry=r)\n",
        "\n",
        "  H_const, H_obt, H_tbt = get_chem_tensors(H)\n",
        "  H_obt_op, H_tbt_op = obt2op(H_obt), tbt2op(H_tbt)\n",
        "  Hchem = H_obt_op + H_tbt_op + H_const\n",
        "\n",
        "  for j, method in enumerate(all_methods):\n",
        "    if j < 4:\n",
        "      if (mol_name != 'nh3'):\n",
        "        JW_OF = jordan_wigner(Hchem)\n",
        "        n_qubits = count_qubits(JW_OF)\n",
        "        already_tapered = False\n",
        "\n",
        "      #Perform tapering if it is nh3 and method is a qubit method\n",
        "      if (mol_name == 'nh3') and (not already_tapered):\n",
        "        JW_OF = taper_qubits(JW_OF, n_qubits, int(num_elecs/2), int(num_elecs/2))\n",
        "        n_qubits = count_qubits(JW_OF)\n",
        "        already_tapered = True\n",
        "\n",
        "      JW_keys = JW_OF.terms.keys()\n",
        "      Nr = 2*len(JW_keys)                                                     # *2 because we are working with 2nd order Trotter\n",
        "    else:\n",
        "      try:\n",
        "        with open(savepath + f\"{method}/sparse arrays/Full space/{mol_name}_sarray.pkl\", 'rb') as in_file:\n",
        "          frags = pickle.load(in_file)\n",
        "      except FileNotFoundError:\n",
        "        frags = load_h5_list_of_sparse(savepath + f\"{method}/sparse arrays/Full space/{mol_name}_sarray.h5\")\n",
        "      n_qubits = count_qubits(H)\n",
        "      Nr = countOneRotFerm(n_qubits,len(frags), ord=2)\n",
        "\n",
        "    print (f\"Currently doing {mol_name} with {method}\")\n",
        "\n",
        "    err_est = eps_v2_exact[i, j]\n",
        "    global_optimal_errs, global_min_Nt = explore_params(n_samps, Tot_Err, err_est, Nr, scale, use_constraints = True)\n",
        "    if sum(global_optimal_errs) - Tot_Err > 1e-8:\n",
        "      raise ValueError(\"Sum of global_optimal_errs exceeds Tot_Err\")\n",
        "    eps_v2_exact_T_count[i, j] = global_min_Nt\n",
        "\n",
        "    err_est = eps_v2_app[i, j]\n",
        "    global_optimal_errs, global_min_Nt = explore_params(n_samps, Tot_Err, err_est, Nr, scale, use_constraints = True)\n",
        "    if sum(global_optimal_errs) - Tot_Err > 1e-8:\n",
        "      raise ValueError(\"Sum of global_optimal_errs exceeds Tot_Err\")\n",
        "    eps_v2_app_T_count[i, j] = global_min_Nt\n",
        "\n",
        "    err_est = alpha[i, j]\n",
        "    global_optimal_errs, global_min_Nt = explore_params(n_samps, Tot_Err, err_est, Nr, scale, use_constraints = False)\n",
        "    if sum(global_optimal_errs) - Tot_Err > 1e-8:\n",
        "      raise ValueError(\"Sum of global_optimal_errs exceeds Tot_Err\")\n",
        "    alpha_T_count[i, j] = global_min_Nt\n",
        "\n",
        "    print (f\"Completed T gate count for {mol_name} with {method} \\n\")\n",
        "\n",
        "\n",
        "np.set_printoptions(linewidth=95)\n",
        "np.set_printoptions(formatter={'float': '{:.2e}'.format})\n",
        "print (np.matrix(alpha_T_count))\n",
        "print (np.matrix(eps_v2_exact_T_count))\n",
        "print (np.matrix(eps_v2_app_T_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing and comparing the best methods based on T gate estimates"
      ],
      "metadata": {
        "id": "ySpyjiN1_kbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(linewidth=95)\n",
        "np.set_printoptions(formatter={'float': '{:.2e}'.format})\n",
        "# print (np.matrix(alpha_T_count))\n",
        "# print (np.matrix(eps_v2_exact_T_count))\n",
        "# print (np.matrix(eps_v2_app_T_count))\n",
        "\n",
        "\n",
        "methods = ['qwc lf', 'qwc si', 'fc lf', 'fc si', 'lr lcu', 'gfro lcu', 'lr', 'gfro', 'sd gfro']\n",
        "mol_names = ['h2', 'lih', 'beh2', 'h2o', 'nh3']\n",
        "\n",
        "alpha_T_count = np.array([\n",
        "    [1.56e+07, 1.40e+07, 1.65e+07, 1.40e+07, 1.39e+08, 1.05e+08, 1.35e+08, 1.01e+08, 2.76e+08],\n",
        "    [6.22e+09, 2.99e+09, 4.71e+09, 2.98e+09, 2.31e+10, 5.10e+10, 4.75e+10, 1.04e+11, 1.66e+11],\n",
        "    [1.34e+10, 6.42e+09, 1.46e+10, 6.30e+09, 9.17e+10, 2.21e+11, 1.89e+11, 4.35e+11, 7.31e+11],\n",
        "    [1.02e+11, 6.02e+10, 1.56e+11, 5.93e+10, 4.63e+11, 1.06e+12, 8.75e+11, 1.94e+12, 2.83e+12],\n",
        "    [8.16e+10, 4.50e+10, 9.27e+10, 4.45e+10, 5.46e+11, 1.60e+12, 1.06e+12, 3.01e+12, 3.89e+12]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "eps_v2_exact_T_count = np.array([\n",
        "    [6.22e+06, 6.22e+06, 6.22e+06, 6.22e+06, 6.19e+07, 4.65e+07, 5.70e+07, 4.29e+07, 1.20e+08],\n",
        "    [3.13e+08, 2.53e+08, 3.01e+08, 2.70e+08, 3.52e+09, 8.32e+09, 1.40e+10, 3.36e+10, 4.58e+10],\n",
        "    [7.01e+08, 6.25e+08, 9.11e+08, 5.55e+08, 1.10e+10, 2.79e+10, 1.98e+10, 5.31e+10, 9.21e+10],\n",
        "    [7.72e+08, 6.72e+08, 1.55e+09, 5.14e+08, 1.66e+10, 3.97e+10, 4.21e+10, 9.25e+10, 7.83e+10],\n",
        "    [1.06e+09, 9.88e+08, 3.09e+09, 1.23e+09, 2.55e+10, 7.56e+10, 7.61e+10, 2.06e+11, 1.62e+11]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "eps_v2_app_T_count = np.array([\n",
        "    [6.22e+06, 6.22e+06, 6.22e+06, 6.22e+06, 6.19e+07, 4.65e+07, 5.70e+07, 4.29e+07, 1.20e+08],\n",
        "    [3.13e+08, 2.54e+08, 3.01e+08, 2.70e+08, 3.52e+09, 8.32e+09, 1.40e+10, 3.36e+10, 4.58e+10],\n",
        "    [7.04e+08, 6.29e+08, 9.07e+08, 5.60e+08, 1.11e+10, 2.82e+10, 1.99e+10, 5.33e+10, 9.26e+10],\n",
        "    [9.78e+08, 8.83e+08, 1.41e+09, 7.60e+08, 1.95e+10, 4.73e+10, 4.73e+10, 1.06e+11, 9.14e+10],\n",
        "    [1.14e+09, 1.25e+09, 2.88e+09, 1.01e+09, 3.32e+10, 1.01e+11, 9.11e+10, 2.56e+11, 2.07e+11]\n",
        "])\n",
        "\n",
        "best_alpha_T_count = np.zeros((5, 3))\n",
        "best_eps_v2_exact_T_count = np.zeros((5, 3))\n",
        "best_eps_v2_app_T_count = np.zeros((5, 3))\n",
        "\n",
        "n=3                                                                     #Choose best 3 fragmentation\n",
        "for i in range (5):\n",
        "  print (f\"Molecule = {mol_names[i]}\")\n",
        "\n",
        "  eps_v2_exact_T_count_i = eps_v2_exact_T_count[i, :]\n",
        "  # Step 1: Get indices\n",
        "  smallest_indices = np.argpartition(eps_v2_exact_T_count_i, n)[:n]\n",
        "  # Step 2: Sort those indices by actual value\n",
        "  smallest_indices = smallest_indices[np.argsort(eps_v2_exact_T_count_i[smallest_indices])]\n",
        "  # Step 3: Get the corresponding smallest values\n",
        "  smallest_values = eps_v2_exact_T_count_i[smallest_indices]\n",
        "  best_eps_v2_exact_T_count[i, :] = smallest_values\n",
        "  print ([methods[smallest_indices[j]].upper() + \" $({:.2e}\".format(smallest_values[j])[:6]+'\\times 10^{'+\"{:.2e}\".format(smallest_values[j])[-2:]+\"})$\" for j in range(3)])\n",
        "\n",
        "  alpha_T_count_i = alpha_T_count[i, :]\n",
        "  # Step 1: Get indices of n smallest values (unsorted)\n",
        "  smallest_indices = np.argpartition(alpha_T_count_i, n)[:n]\n",
        "  # Step 2: Sort those indices by actual value\n",
        "  smallest_indices = smallest_indices[np.argsort(alpha_T_count_i[smallest_indices])]\n",
        "  # Step 3: Get the corresponding smallest values\n",
        "  smallest_values = alpha_T_count_i[smallest_indices]\n",
        "  best_alpha_T_count[i, :] = smallest_values\n",
        "  print ([methods[smallest_indices[j]].upper() + \" $({:.2e}\".format(smallest_values[j])[:6]+'\\times 10^{'+\"{:.2e}\".format(smallest_values[j])[-2:]+\"})$\" for j in range(3)])\n",
        "\n",
        "\n",
        "  eps_v2_app_T_count_i = eps_v2_app_T_count[i, :]\n",
        "  # Step 1: Get indices\n",
        "  smallest_indices = np.argpartition(eps_v2_app_T_count_i, n)[:n]\n",
        "  # Step 2: Sort those indices by actual value\n",
        "  smallest_indices = smallest_indices[np.argsort(eps_v2_app_T_count_i[smallest_indices])]\n",
        "  # Step 3: Get the corresponding smallest values\n",
        "  smallest_values = eps_v2_app_T_count_i[smallest_indices]\n",
        "  best_eps_v2_app_T_count[i, :] = smallest_values\n",
        "  print ([methods[smallest_indices[j]].upper() + \" $({:.2e}\".format(smallest_values[j])[:6]+'\\times 10^{'+\"{:.2e}\".format(smallest_values[j])[-2:]+\"})$\" for j in range(3)])\n",
        "  print ('\\n\\n')\n",
        "\n",
        "# print (best_alpha_T_count)\n",
        "# print (best_eps_v2_exact_T_count)\n",
        "# print (best_eps_v2_app_T_count)"
      ],
      "metadata": {
        "id": "KKBUO8sJob0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OIDFjVl5knFg",
        "mriltbl1l5Pt",
        "lvXddtgk74Qh"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}